# # Training hyperparameters and toggles
# dataset: cifar10              # options: cifar10, cifar100, stl10
# epochs: 10
# batch_size: 64
# lr: 0.001
# weight_decay: 0.01

# # Training toggles
# use_amp: True                 # mixed precision training
# use_scheduler: True           # cosine annealing LR scheduler
# use_grad_clip: True           # gradient clipping
# freeze_backbone: False        # freeze backbone layers (can override model.yaml)
# pretrained: True              # use pretrained weights (can override model.yaml)
# seed: 42                      # for reproducibility
# num_workers: 2                # dataloader workers


# train.yaml for CPU
dataset: cifar10
epochs: 3
batch_size: 16
lr: 0.001
weight_decay: 0.0
use_amp: False          # AMP not needed on CPU
use_scheduler: False
use_grad_clip: False
freeze_backbone: False
pretrained: False
seed: 42
num_workers: 0
